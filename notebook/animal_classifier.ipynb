{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 動物画像分類システム - CNN実装プロジェクト\n",
    "\n",
    "## プロジェクト内容\n",
    "\n",
    "畳み込みニューラルネットワーク（CNN）を使用した猫と犬の画像分類システムです。TensorFlow Datasetsから取得した高品質な動物画像データを用いて、深層学習による画像認識モデルを構築しました。TensorFlowによるCNN実装とコンピュータビジョン技術を学習することを目的として開発しました。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 06:37:24.886482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "フォルダを作成しました: /Users/y_suzuki/Documents/PythonPortfolioProject/DeepLearning/animal_image_classifier/image_datasets/training_images/cats\n",
      "フォルダを作成しました: /Users/y_suzuki/Documents/PythonPortfolioProject/DeepLearning/animal_image_classifier/image_datasets/training_images/dogs\n",
      "フォルダを作成しました: /Users/y_suzuki/Documents/PythonPortfolioProject/DeepLearning/animal_image_classifier/image_datasets/test_images/cats\n",
      "フォルダを作成しました: /Users/y_suzuki/Documents/PythonPortfolioProject/DeepLearning/animal_image_classifier/image_datasets/test_images/dogs\n",
      "フォルダを作成しました: /Users/y_suzuki/Documents/PythonPortfolioProject/DeepLearning/animal_image_classifier/image_datasets/prediction_samples\n",
      "猫と犬の画像データセットをダウンロード中...\n",
      "データセット名: cats_vs_dogs\n",
      "総画像数: 23262\n",
      "クラス数: 2\n",
      "クラス名: ['cat', 'dog']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 06:37:56.484109: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "利用可能な猫の画像: 11658枚\n",
      "利用可能な犬の画像: 11604枚\n",
      "訓練用猫画像を8000枚保存しました\n",
      "訓練用犬画像を8000枚保存しました\n",
      "テスト用猫画像を2000枚保存しました\n",
      "テスト用犬画像を2000枚保存しました\n",
      "猫のサンプル画像を保存しました: /Users/y_suzuki/Documents/PythonPortfolioProject/DeepLearning/animal_image_classifier/image_datasets/prediction_samples/sample_cat.jpg\n",
      "犬のサンプル画像を保存しました: /Users/y_suzuki/Documents/PythonPortfolioProject/DeepLearning/animal_image_classifier/image_datasets/prediction_samples/sample_dog.jpg\n",
      "\n",
      "データセットの準備が完了しました！\n",
      "訓練用画像: 猫8000枚、犬8000枚\n",
      "テスト用画像: 猫2000枚、犬2000枚\n",
      "予測用サンプル: 各クラス1枚ずつ\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 警告メッセージを非表示にする\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# プロジェクトのベースディレクトリを設定\n",
    "base_dir = os.path.dirname(os.getcwd())\n",
    "dataset_dir = os.path.join(base_dir, 'image_datasets')\n",
    "\n",
    "# データセット用フォルダ構造を作成\n",
    "folders = [\n",
    "    'training_images/cats',\n",
    "    'training_images/dogs', \n",
    "    'test_images/cats',\n",
    "    'test_images/dogs',\n",
    "    'prediction_samples'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(dataset_dir, folder)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    print(f\"フォルダを作成しました: {folder_path}\")\n",
    "\n",
    "# TensorFlow Datasetsから猫と犬のデータセットをダウンロード\n",
    "print(\"猫と犬の画像データセットをダウンロード中...\")\n",
    "dataset, info = tfds.load(\n",
    "    'cats_vs_dogs',\n",
    "    with_info=True,\n",
    "    as_supervised=True,\n",
    "    data_dir=os.path.join(base_dir, 'tensorflow_datasets')\n",
    ")\n",
    "\n",
    "# データセット情報を表示\n",
    "print(f\"データセット名: {info.name}\")\n",
    "print(f\"総画像数: {info.splits['train'].num_examples}\")\n",
    "print(f\"クラス数: {info.features['label'].num_classes}\")\n",
    "print(f\"クラス名: {info.features['label'].names}\")\n",
    "\n",
    "# データセットを取得\n",
    "train_dataset = dataset['train']\n",
    "\n",
    "def save_images_from_dataset(dataset, save_path, class_name, target_class, max_count):\n",
    "    \"\"\"データセットから指定されたクラスの画像を保存する関数\"\"\"\n",
    "    count = 0\n",
    "    for image, label in dataset:\n",
    "        if count >= max_count:\n",
    "            break\n",
    "        \n",
    "        if label.numpy() == target_class:\n",
    "            # 画像を0-255の範囲に変換\n",
    "            image_array = tf.cast(image, tf.uint8).numpy()\n",
    "            \n",
    "            # PIL画像として保存\n",
    "            pil_image = Image.fromarray(image_array)\n",
    "            \n",
    "            # ファイル名を生成\n",
    "            filename = f\"{class_name}_{count+1:04d}.jpg\"\n",
    "            filepath = os.path.join(save_path, filename)\n",
    "            \n",
    "            # 画像を保存\n",
    "            pil_image.save(filepath)\n",
    "            count += 1\n",
    "    \n",
    "    print(f\"{class_name}の画像を{count}枚保存しました\")\n",
    "    return count\n",
    "\n",
    "# データセットをリストに変換（効率的な処理のため）\n",
    "dataset_list = list(train_dataset.take(25000))  # 全データセットを取得\n",
    "np.random.shuffle(dataset_list)  # シャッフル\n",
    "\n",
    "# 猫の画像を分離（ラベル0が猫）\n",
    "cat_images = [(img, lbl) for img, lbl in dataset_list if lbl.numpy() == 0]\n",
    "# 犬の画像を分離（ラベル1が犬）\n",
    "dog_images = [(img, lbl) for img, lbl in dataset_list if lbl.numpy() == 1]\n",
    "\n",
    "print(f\"利用可能な猫の画像: {len(cat_images)}枚\")\n",
    "print(f\"利用可能な犬の画像: {len(dog_images)}枚\")\n",
    "\n",
    "# 訓練用猫画像を保存（8000枚）\n",
    "train_cats_path = os.path.join(dataset_dir, 'training_images', 'cats')\n",
    "saved_count = 0\n",
    "for i, (image, label) in enumerate(cat_images[:8000]):\n",
    "    image_array = tf.cast(image, tf.uint8).numpy()\n",
    "    pil_image = Image.fromarray(image_array)\n",
    "    filename = f\"cat_{saved_count+1:04d}.jpg\"\n",
    "    filepath = os.path.join(train_cats_path, filename)\n",
    "    pil_image.save(filepath)\n",
    "    saved_count += 1\n",
    "\n",
    "print(f\"訓練用猫画像を{saved_count}枚保存しました\")\n",
    "\n",
    "# 訓練用犬画像を保存（8000枚）\n",
    "train_dogs_path = os.path.join(dataset_dir, 'training_images', 'dogs')\n",
    "saved_count = 0\n",
    "for i, (image, label) in enumerate(dog_images[:8000]):\n",
    "    image_array = tf.cast(image, tf.uint8).numpy()\n",
    "    pil_image = Image.fromarray(image_array)\n",
    "    filename = f\"dog_{saved_count+1:04d}.jpg\"\n",
    "    filepath = os.path.join(train_dogs_path, filename)\n",
    "    pil_image.save(filepath)\n",
    "    saved_count += 1\n",
    "\n",
    "print(f\"訓練用犬画像を{saved_count}枚保存しました\")\n",
    "\n",
    "# テスト用猫画像を保存（2000枚）\n",
    "test_cats_path = os.path.join(dataset_dir, 'test_images', 'cats')\n",
    "saved_count = 0\n",
    "for i, (image, label) in enumerate(cat_images[8000:10000]):\n",
    "    image_array = tf.cast(image, tf.uint8).numpy()\n",
    "    pil_image = Image.fromarray(image_array)\n",
    "    filename = f\"cat_{saved_count+1:04d}.jpg\"\n",
    "    filepath = os.path.join(test_cats_path, filename)\n",
    "    pil_image.save(filepath)\n",
    "    saved_count += 1\n",
    "\n",
    "print(f\"テスト用猫画像を{saved_count}枚保存しました\")\n",
    "\n",
    "# テスト用犬画像を保存（2000枚）\n",
    "test_dogs_path = os.path.join(dataset_dir, 'test_images', 'dogs')\n",
    "saved_count = 0\n",
    "for i, (image, label) in enumerate(dog_images[8000:10000]):\n",
    "    image_array = tf.cast(image, tf.uint8).numpy()\n",
    "    pil_image = Image.fromarray(image_array)\n",
    "    filename = f\"dog_{saved_count+1:04d}.jpg\"\n",
    "    filepath = os.path.join(test_dogs_path, filename)\n",
    "    pil_image.save(filepath)\n",
    "    saved_count += 1\n",
    "\n",
    "print(f\"テスト用犬画像を{saved_count}枚保存しました\")\n",
    "\n",
    "# 予測用サンプル画像を作成\n",
    "prediction_samples_dir = os.path.join(dataset_dir, 'prediction_samples')\n",
    "\n",
    "# 猫のサンプル画像\n",
    "if len(cat_images) > 10000:\n",
    "    sample_cat_image, _ = cat_images[10000]\n",
    "    cat_array = tf.cast(sample_cat_image, tf.uint8).numpy()\n",
    "    cat_sample_path = os.path.join(prediction_samples_dir, 'sample_cat.jpg')\n",
    "    Image.fromarray(cat_array).save(cat_sample_path)\n",
    "    print(f\"猫のサンプル画像を保存しました: {cat_sample_path}\")\n",
    "\n",
    "# 犬のサンプル画像\n",
    "if len(dog_images) > 10000:\n",
    "    sample_dog_image, _ = dog_images[10000]\n",
    "    dog_array = tf.cast(sample_dog_image, tf.uint8).numpy()\n",
    "    dog_sample_path = os.path.join(prediction_samples_dir, 'sample_dog.jpg')\n",
    "    Image.fromarray(dog_array).save(dog_sample_path)\n",
    "    print(f\"犬のサンプル画像を保存しました: {dog_sample_path}\")\n",
    "\n",
    "print(\"\\nデータセットの準備が完了しました！\")\n",
    "print(f\"訓練用画像: 猫8000枚、犬8000枚\")\n",
    "print(f\"テスト用画像: 猫2000枚、犬2000枚\")\n",
    "print(f\"予測用サンプル: 各クラス1枚ずつ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディープラーニングフレームワーク\n",
    "import tensorflow as tf\n",
    "# 画像データ前処理のためのライブラリ\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.16.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlowのバージョンを確認\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練用画像データの前処理設定を作成\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,           # ピクセル値を0-1の範囲に正規化\n",
    "    shear_range=0.2,          # シアー変換（画像の歪み）を最大20%適用\n",
    "    zoom_range=0.2,           # ズーム変換を最大20%適用  \n",
    "    horizontal_flip=True      # 水平反転をランダムに適用\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 訓練用画像データを読み込み、前処理を適用\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    '../image_datasets/training_images',  # 訓練画像が格納されているディレクトリ\n",
    "    target_size=(64, 64),                 # 画像サイズを64x64ピクセルにリサイズ\n",
    "    batch_size=32,                        # 1回に処理する画像数を32枚に設定\n",
    "    class_mode='binary'                   # 二値分類（猫と犬）を指定\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用画像データの前処理設定を作成（データ拡張なし）\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)  # 正規化のみ適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# テスト用画像データを読み込み、前処理を適用\n",
    "test_set = test_datagen.flow_from_directory(\n",
    "    '../image_datasets/test_images',      # テスト画像が格納されているディレクトリ\n",
    "    target_size=(64, 64),                 # 画像サイズを64x64ピクセルにリサイズ\n",
    "    batch_size=32,                        # 1回に処理する画像数を32枚に設定\n",
    "    class_mode='binary'                   # 二値分類（猫と犬）を指定\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequentialモデルで畳み込みニューラルネットワークを初期化\n",
    "cnn = tf.keras.models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第1畳み込み層を追加\n",
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32,                    # フィルター（特徴検出器）の数を32個に設定\n",
    "    kernel_size=3,                 # カーネル（フィルター）のサイズを3x3に設定\n",
    "    activation='relu',             # 活性化関数にReLUを使用\n",
    "    input_shape=[64, 64, 3]       # 入力画像の形状（64x64ピクセル、3チャンネル）\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第1最大プーリング層を追加\n",
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=2,      # プーリング窓のサイズを2x2に設定\n",
    "    strides=2         # プーリング窓の移動幅を2ピクセルに設定\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第2畳み込み層を追加\n",
    "cnn.add(tf.keras.layers.Conv2D(\n",
    "    filters=32,            # フィルター数を32個に設定\n",
    "    kernel_size=3,         # カーネルサイズを3x3に設定\n",
    "    activation='relu'      # 活性化関数にReLUを使用\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第2最大プーリング層を追加\n",
    "cnn.add(tf.keras.layers.MaxPool2D(\n",
    "    pool_size=2,      # プーリング窓のサイズを2x2に設定\n",
    "    strides=2         # プーリング窓の移動幅を2ピクセルに設定\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# フラット化層を追加（2次元の特徴マップを1次元ベクトルに変換）\n",
    "cnn.add(tf.keras.layers.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全結合隠れ層を追加\n",
    "cnn.add(tf.keras.layers.Dense(\n",
    "    units=128,             # ニューロン数を128個に設定\n",
    "    activation='relu'      # 活性化関数にReLUを使用\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力層を追加\n",
    "cnn.add(tf.keras.layers.Dense(\n",
    "    units=1,               # 出力ニューロン数を1個に設定（二値分類のため）\n",
    "    activation='sigmoid'   # 活性化関数にSigmoidを使用\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNをコンパイル（学習の設定を行う）\n",
    "cnn.compile(\n",
    "    optimizer='adam',              # 最適化アルゴリズムにAdamを使用\n",
    "    loss='binary_crossentropy',   # 損失関数に二値交差エントロピーを使用\n",
    "    metrics=['accuracy']           # 評価指標に精度を使用\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 222ms/step - accuracy: 0.6193 - loss: 0.6450 - val_accuracy: 0.7385 - val_loss: 0.5200\n",
      "Epoch 2/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 226ms/step - accuracy: 0.7270 - loss: 0.5370 - val_accuracy: 0.7675 - val_loss: 0.4816\n",
      "Epoch 3/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 211ms/step - accuracy: 0.7604 - loss: 0.4917 - val_accuracy: 0.7800 - val_loss: 0.4612\n",
      "Epoch 4/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 191ms/step - accuracy: 0.7765 - loss: 0.4590 - val_accuracy: 0.7868 - val_loss: 0.4478\n",
      "Epoch 5/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 202ms/step - accuracy: 0.7932 - loss: 0.4409 - val_accuracy: 0.7825 - val_loss: 0.4496\n",
      "Epoch 6/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 194ms/step - accuracy: 0.8035 - loss: 0.4215 - val_accuracy: 0.8008 - val_loss: 0.4290\n",
      "Epoch 7/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 195ms/step - accuracy: 0.8105 - loss: 0.4152 - val_accuracy: 0.8120 - val_loss: 0.4146\n",
      "Epoch 8/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 202ms/step - accuracy: 0.8237 - loss: 0.3873 - val_accuracy: 0.8092 - val_loss: 0.4194\n",
      "Epoch 9/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 215ms/step - accuracy: 0.8344 - loss: 0.3702 - val_accuracy: 0.8140 - val_loss: 0.4014\n",
      "Epoch 10/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 196ms/step - accuracy: 0.8334 - loss: 0.3694 - val_accuracy: 0.8025 - val_loss: 0.4659\n",
      "Epoch 11/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 205ms/step - accuracy: 0.8489 - loss: 0.3456 - val_accuracy: 0.8265 - val_loss: 0.3925\n",
      "Epoch 12/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 202ms/step - accuracy: 0.8577 - loss: 0.3315 - val_accuracy: 0.7940 - val_loss: 0.4775\n",
      "Epoch 13/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 198ms/step - accuracy: 0.8611 - loss: 0.3273 - val_accuracy: 0.8240 - val_loss: 0.4150\n",
      "Epoch 14/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 208ms/step - accuracy: 0.8688 - loss: 0.3088 - val_accuracy: 0.8305 - val_loss: 0.4027\n",
      "Epoch 15/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 210ms/step - accuracy: 0.8616 - loss: 0.3032 - val_accuracy: 0.8350 - val_loss: 0.3913\n",
      "Epoch 16/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 206ms/step - accuracy: 0.8803 - loss: 0.2840 - val_accuracy: 0.8292 - val_loss: 0.4022\n",
      "Epoch 17/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 207ms/step - accuracy: 0.8879 - loss: 0.2679 - val_accuracy: 0.8100 - val_loss: 0.4556\n",
      "Epoch 18/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 207ms/step - accuracy: 0.8979 - loss: 0.2500 - val_accuracy: 0.8365 - val_loss: 0.4051\n",
      "Epoch 19/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 202ms/step - accuracy: 0.8956 - loss: 0.2564 - val_accuracy: 0.8288 - val_loss: 0.4081\n",
      "Epoch 20/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 204ms/step - accuracy: 0.9048 - loss: 0.2401 - val_accuracy: 0.7903 - val_loss: 0.5598\n",
      "Epoch 21/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 207ms/step - accuracy: 0.9014 - loss: 0.2324 - val_accuracy: 0.8335 - val_loss: 0.4325\n",
      "Epoch 22/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 203ms/step - accuracy: 0.9128 - loss: 0.2112 - val_accuracy: 0.8303 - val_loss: 0.4445\n",
      "Epoch 23/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 197ms/step - accuracy: 0.9220 - loss: 0.2001 - val_accuracy: 0.8295 - val_loss: 0.4380\n",
      "Epoch 24/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 197ms/step - accuracy: 0.9246 - loss: 0.1931 - val_accuracy: 0.8285 - val_loss: 0.4689\n",
      "Epoch 25/25\n",
      "\u001b[1m500/500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 200ms/step - accuracy: 0.9256 - loss: 0.1861 - val_accuracy: 0.8385 - val_loss: 0.4400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x110ecc990>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 訓練データでモデルを学習し、テストデータで同時評価\n",
    "cnn.fit(\n",
    "    x=training_set,              # 学習に使用する訓練データ\n",
    "    validation_data=test_set,    # 各エポック終了時に評価を行うテストデータ\n",
    "    epochs=25                    # 学習回数（エポック数）を25回に設定\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数値計算用ライブラリ\n",
    "import numpy as np\n",
    "# 画像処理用ライブラリ\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測用画像を読み込み（猫のサンプル画像を使用）\n",
    "test_image = image.load_img(\n",
    "    '../image_datasets/prediction_samples/sample_cat.jpg',  # 予測対象の画像ファイルパス\n",
    "    target_size=(64, 64)                                    # 画像サイズを64x64に調整\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIL画像をNumPy配列に変換\n",
    "test_image = image.img_to_array(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# バッチ次元を追加（モデルはバッチ入力を期待するため）\n",
    "test_image = np.expand_dims(test_image, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n"
     ]
    }
   ],
   "source": [
    "# CNNモデルで予測を実行\n",
    "result = cnn.predict(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# クラスインデックスを確認（どちらが猫でどちらが犬かを確認）\n",
    "training_set.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "# 予測結果を二値分類で判定\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'      # 結果が1なら犬と判定\n",
    "else:\n",
    "    prediction = 'cat'      # 結果が0なら猫と判定\n",
    "\n",
    "# 予測結果を表示\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
